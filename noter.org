#+TITLE:Parallel functional programming

* 16/11 Deterministic parallel programming and data parallelism
Same motivation as PMPH. Moore is slowing down, but thread count is greater than ever

- Task Parallelism
simultaneously performing different operations on potentially different pieces of data
/threeeeads/

- Data parallelism
Simultaneously performing the same operation on different pieces of the same data

** Data parallelism
If x and y are vectors, then $x + y$ is a data parallel operation  
- Numpy, R, Matlab, julia, SQL are all data parallel  
These libraries make data parallel programming so easy, so noone actually thinks about parallelism  

They are used in a sequential way, so it's seamless.

*** Numpy example
#+BEGIN_SRC python
  import numpy as np
  def inc_scalar(x):
  """We state exactly which x entry should be handled
     e.g. i = 0, 1, 2, ... #x
    """
      for i in range(len(x)):
	  x[i] = x[i] + 1
  def inc_par(x):
  """We don't state which x entry should be handled first"""
      return x + np.ones(x.shape)
#+END_SRC

You get the speedup by calling numpy primitives, which are implemented on the backend in parallel C or FORTRAN

=inc_par= is purely functional, having no side effects or assignments.

Here is a sum of arrays, implemented in a parallel fashion. We used divide and conquer
#+BEGIN_SRC python
  def sum_pow2(x):
      while len(x) > 0:
	  x_first = x[0:len(x) / 2]
	  x_last = x[len(x)/2:]
	  x = x_first + x_last
      return x[0]
#+END_SRC
We did the following transformation
\[ (((((((x_0 + x_1) + x_2) + x_3) + x_4) + x_5) + x_6) + x_7) \]
to
\[ (((x_0 + x_1) + (x_2 + x_3)) + ((x_4 + x_5) + (x_6 + x_7))) \]

Which is ok because it holds the following:  
- The binop must be cumulative
- The binop must be associative
- The binop must have a neutral element

Monoids hold parallel also:  
/An associative binary operator with a neutral element is called a monoid and written/ =(binop, neutral)=   
Examples: 
- (+, 0)
- (*, 1)
- (++, [])

These are often implemented using =reduce=. We define summation as =reduce (+) 0=

*** The problem with control flow
It is awkward to encode per-item control flow in a first-order parallel language. E.g
=y[i] = sqrt(x[i]) if x[i] > 0 else 0=

We try to argue about that parallel
#+BEGIN_SRC python
  def sqrt_when_pos_1(x):
      x = x.copy()
      x_nonneg = x >= 0 #flag array
      x[x_nonneg] = np.sqrt(x[x_nonneg])
      return x

  """
  x           [ 1, 2, -3]
  x_nonneg    [ 1, 1,  0]
  x[x_nonneg] [ 1, 2]
  """
#+END_SRC
Parallel filtering is not so simple :'(

*** Well what about while loops?
The mandelbrot set. A popular way of printing pretty pictures
#+BEGIN_SRC python
  def divergence (c, d):
      i = 0
      z = c
      while i < d and dot(z) < 4.0:
	  z = c + z * z
	  i = i + 1
      return 1
#+END_SRC

This isn't easily parallelizable. Lets check that out in numpy

#+BEGIN_SRC python
  def mandelbrot_numpy(c, d):
      """ c is an array of complex numbers. D is still just depth scalar"""
      output = np.zeros(c.shape)
      z = np.zeros(c.shape, np.complex32)
      for it in range(d):
	  notdone = ( # Flag array
	      z.real * z.real + z.imag * z.imag
	      ) < 4.0
	  output[notdone] = it
	  z[notdone] = z[notdone] ** 2 + c[notdone]
      return output
#+END_SRC
We calculate the flag array =notdone= with the squared z < 4. We update the output
sets for those who haven't finished. 

Problems:
- Control flow is hidden, and code is needlessly complex
- Always runs in =d= iterations, instead of premature completion
- /Lots/ of memory traffic

** What about futhark
I kind of zoned out here. Futhark goes wheeee
#+BEGIN_SRC futhark
  let divergence (c: complex) (d: i32): i32 = 
    let (_, i) =
      loop (z, i) = (c, 0)
      while i < d && dot(z) < 4.0 do
	(addComplex c (multComplex z z), i + 1)
    in i'
  let mandelbrot [n][m] (css: [n][m]complex)
			(d: i32) : [n][m]i32 =
    map (\cs -> map (\c -> divergence c d) cs) css
#+END_SRC

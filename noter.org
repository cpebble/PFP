#+TITLE:Parallel functional programming

* 16/11 Deterministic parallel programming and data parallelism
Same motivation as PMPH. Moore is slowing down, but thread count is greater than ever

- Task Parallelism
simultaneously performing different operations on potentially different pieces of data
/threeeeads/

- Data parallelism
Simultaneously performing the same operation on different pieces of the same data

** Data parallelism
If x and y are vectors, then $x + y$ is a data parallel operation  
- Numpy, R, Matlab, julia, SQL are all data parallel  
These libraries make data parallel programming so easy, so noone actually thinks about parallelism  

They are used in a sequential way, so it's seamless.

*** Numpy example
#+BEGIN_SRC python
  import numpy as np
  def inc_scalar(x):
  """We state exactly which x entry should be handled
     e.g. i = 0, 1, 2, ... #x
    """
      for i in range(len(x)):
	  x[i] = x[i] + 1
  def inc_par(x):
  """We don't state which x entry should be handled first"""
      return x + np.ones(x.shape)
#+END_SRC

You get the speedup by calling numpy primitives, which are implemented on the backend in parallel C or FORTRAN

=inc_par= is purely functional, having no side effects or assignments.

Here is a sum of arrays, implemented in a parallel fashion. We used divide and conquer
#+BEGIN_SRC python
  def sum_pow2(x):
      while len(x) > 0:
	  x_first = x[0:len(x) / 2]
	  x_last = x[len(x)/2:]
	  x = x_first + x_last
      return x[0]
#+END_SRC
We did the following transformation
\[ (((((((x_0 + x_1) + x_2) + x_3) + x_4) + x_5) + x_6) + x_7) \]
to
\[ (((x_0 + x_1) + (x_2 + x_3)) + ((x_4 + x_5) + (x_6 + x_7))) \]

Which is ok because it holds the following:  
- The binop must be cumulative
- The binop must be associative
- The binop must have a neutral element

Monoids hold parallel also:  
/An associative binary operator with a neutral element is called a monoid and written/ =(binop, neutral)=   
Examples: 
- (+, 0)
- (*, 1)
- (++, [])

These are often implemented using =reduce=. We define summation as =reduce (+) 0=

*** The problem with control flow
It is awkward to encode per-item control flow in a first-order parallel language. E.g
=y[i] = sqrt(x[i]) if x[i] > 0 else 0=

We try to argue about that parallel
#+BEGIN_SRC python
  def sqrt_when_pos_1(x):
      x = x.copy()
      x_nonneg = x >= 0 #flag array
      x[x_nonneg] = np.sqrt(x[x_nonneg])
      return x

  """
  x           [ 1, 2, -3]
  x_nonneg    [ 1, 1,  0]
  x[x_nonneg] [ 1, 2]
  """
#+END_SRC
Parallel filtering is not so simple :'(

*** Well what about while loops?
The mandelbrot set. A popular way of printing pretty pictures
#+BEGIN_SRC python
  def divergence (c, d):
      i = 0
      z = c
      while i < d and dot(z) < 4.0:
	  z = c + z * z
	  i = i + 1
      return 1
#+END_SRC

This isn't easily parallelizable. Lets check that out in numpy

#+BEGIN_SRC python
  def mandelbrot_numpy(c, d):
      """ c is an array of complex numbers. D is still just depth scalar"""
      output = np.zeros(c.shape)
      z = np.zeros(c.shape, np.complex32)
      for it in range(d):
	  notdone = ( # Flag array
	      z.real * z.real + z.imag * z.imag
	      ) < 4.0
	  output[notdone] = it
	  z[notdone] = z[notdone] ** 2 + c[notdone]
      return output
#+END_SRC
We calculate the flag array =notdone= with the squared z < 4. We update the output
sets for those who haven't finished. 

Problems:
- Control flow is hidden, and code is needlessly complex
- Always runs in =d= iterations, instead of premature completion
- /Lots/ of memory traffic

** What about futhark
I kind of zoned out here. Futhark goes wheeee
#+BEGIN_SRC futhark
  let divergence (c: complex) (d: i32): i32 = 
    let (_, i) =
      loop (z, i) = (c, 0)
      while i < d && dot(z) < 4.0 do
	(addComplex c (multComplex z z), i + 1)
    in i'
  let mandelbrot [n][m] (css: [n][m]complex)
			(d: i32) : [n][m]i32 =
    map (\cs -> map (\c -> divergence c d) cs) css
#+END_SRC
* 18/11 Cost models and advanced Futhark programming
** Cost models
Why?  
Which is better
#+BEGIN_SRC python
  def inc_scalar(x):
      for i in range(len(x)):
	  x[i] = x[i] + 1
  def inc_par(x):
      return x + np.ones(x.shape)
#+END_SRC
The second one, but why? "More parallel" isn't a viable judgement  


Looking at the first function, we get this Diagram:  
=len(x) -> range () -> (x[0], 1) -> x[0] = ... + ... -> ... -> x[n-1] = x[n-1] + 1=

Total count of nodes is the work W(p), the span is $n$(there are n iterations)
The length of the longest path from root to leaf is the span  
With an infinite number of processors, if a program $p$ has the span $k$
written $S(p) = k$, the program can execute in $O(k)$ time.  
Here $W(p) = O(n), S(p) = O(n)$  

Looking at the second function:  
=x.shape -> np.ones() = (np.ones(0), ..., np.ones(n-1)) -> return x + ...=
The work is O(n), the span is O(1)  
W(p) = O(n)  
S(p) = O(1)  

For infinite processors, this would execute in constant time

*** Brents theorem
In practice, we don't have infinite processors.  
The intuition is. If we only have x processors, we can simulate the infinite
processors, by chunking the parallel steps. This gives us an overhead
that is defined by the amount of missing processors

Writing $T_i$ for the time taken to execute an algorithm on $i$
processors, Brent's theorem states that
\[ \frac{T_1}{p} \leq T_p \leq T_{\infty} ... \] 

*** language based
Instead of counting levels in a dag, we use a language-based cost model.  
E.g W(x+y) is defined as W(x) + W(y)

*** Language based cost model for futhark-lang
/We write [e] for the result of evaluating expression /e/

Simple case
#+BEGIN_SRC 
W(v) = 1
S(v) = 1
W(e1 op e2) = W(e1) + W(e2) + 1
// This could be MaX(W(e1) + W(e2)) if we had a guarantee it would be parallel
S(e1 op e2) = S(e1) + S(e2) + 1
W(\x -> e) = 1 // just creating a lambda
S(\x -> e) = 1

W([e_1, ..., e_n]) = W(e_1) + ... + W(e_n) + 1 // array lit
S([e_1, ..., e_n]) = S(e_1) + ... + S(e_n) + 1 
W((e_1, ..., e_n)) = W(e_1) + ... + W(e_n) + 1 // Tuple lit
S((e_1, ..., e_n)) = S(e_1) + ... + S(e_n) + 1 

#+END_SRC
If W = S, it implies no parallelism is guaranteed

Interesting cases:
#+BEGIN_SRC 
W(iota e) = W(e) + [e]
S(iota e) = S(e) + 1 // Implies parallel

W(let x = e in e') = W(e) + W(e'[x->[e]]) + 1
S(let x = e in e') = S(e) + S(e'[x->[e]]) + 1 // No paralllelism
// also tells us that e is fully evaluated before the body

// Long and weird. Depends on evaluating e2 first, then e1
W(e1 e2) = 
S(e1 e2) =

// Map is parallel, but is slowed by the longest function application
W(map e1 e2) = W(e1) + W(e2) + W(e'[x->v1]) + ... + W(e'[x->vn])
S(map e1 e2) = S(e1) + S(e1) + MAX(s(e'[x->v1]) + ... + W(e'[x->vn])
#+END_SRC 

**** Using this, reduction by contraction
#+BEGIN_SRC futhark
  let npow2 (n:i64) : i64 =
    loop a = 2 while a < n do 2*a

  let padpow2 [n] (ne: i32) (v:[n]i32) : []i32 =
    concat v (replicate (npow2 n - n) ne)

  let red (xs : []i32) : i32 =
    let xs =
      loop xs = padpow2 0 xs
      while length xs > 1 do
      let n = length xs / 2
      in map2 (+) xs[0:n] xs[n:2*n]
    in xs[0]
#+END_SRC
What is the Work of loop?
#+BEGIN_SRC 
W(loop x = e_1 while e2 do e3) =
    W(e1) + W(e2[x->[e1]]) + 
    if [e2[x->[e1]] = false then 0
    else W(e2) + W(e3[x->[e1]])+
        W(loop x = [e3[x->[e1]]] while e2 do e3)

Span is the same
#+END_SRC
The work of npow:  
$W(npow2 n) = S(npow2 n) = O(log n)$

The work of padpow:  
=W(padpow(n)) = n=
=S(padpow(n)) = log(n)=
Padpow is parallel  

The work of red W(red xs[n]) =  O(n)
The span of red S(red v) = O(log n)

*** Work efficiency
*A parallel algorithm is said to be work effecient if it has at
most the same work as the best sequential algorithm*

Is red work efficient? Yes.

Is red efficient? /Troels runs them/
It isn't as fast as the built-in reduce

** Prefix sums /scans/
The scan in futhark is inclusive

**** How scans are parallelised
The simplest scan:
#+BEGIN_SRC python
acc = 0
for i < n:
    acc = acc + input[i]
    scanned[i] = acc
#+END_SRC
W(n) = S(n) = O(n)

*Work-efficient scan.*
Includes two passes
- Up-sweep: Build a balanced binary tree of partial sums stored in every other cell
- Down-sweep: Use the tree to fill in the blank
Up sweep values: \[ x^d_i = x^{d+1}_{i-2^{m-d-1}} + x^{d+1}_i \]

We do the down-sweep.  
Please just check the slides for the explanation. It sucks

=W(wescan) = O(n)=
=S(wescan) = log(n)=

** Using scans
*** Filtering
Using scan to filter. Suppose we wish to remove negative elements from the list.  
First we calc a flag array
#+BEGIN_SRC futhark
  let as = [-1, 2, -3, 4, 5, -6]
  let keep = map (\a -> if a >= 0 then 1 else 0) as
  -- [ 0, 1, 0, 1, 1, 0]
  let offsets1 = scan(+) keep
  -- [0, 1, 1, 2, 3, 3]
  let offsets = map(\x -> x - 1)
  -- [-1, 0, 0, 1, 2, 2]

  let res = scatter (replicate (last offsets1) 0)
		    (map2 (\i k -> if k == 1 then i else -1) offsets keep)
		    as
    
#+END_SRC


*** Radix sort
Many sorting algos are a poor fit for data parallelism, but /radix sort/ works well  
Radix-2 works by repeatedly partitioning elements according to one bit at a time,
while preserving the ordering of the previous steps.

Sort by "digits"

#+BEGIN_SRC futhark
  -- Sorting xs: [n] u32
  -- 1 if bit b is set
  let check_bit b x =
    (i64.u32 (x >> u32.i32 b)) & 1


  let bits = map(check_bit b) xs
  let bits_neg = map (1-) bits
  let offs = reduce (x) 0 bits_neg -- How many bits are not set

#+END_SRC
example
| b        | 0             |
| xs       | 0, 1, 2, 3, 4 |
| bits     | 0, 1, 0, 1, 0 |
| bits_neg | 1, 0, 1, 0, 1 |
| offs     | 3             |

continuings
#+BEGIN_SRC futhark
  let idxs0 = map2 (*)
		   bits_neg
		   (scan (+) 0 bits_neg)
  let idxs1 = map2(*)
		  bits
		  (map (+offs) (scan  (+) 0 bits))
		
#+END_SRC


| xs               | 0, 1, 2, 3, 4 |
| bits             | 0, 1, 0, 1, 0 |
| bits_neg         | 1, 0, 1, 0, 1 |
| offs             | 3             |
| idxs0            | 1, 0, 2, 0, 3 |
| idxs1            | 0, 4, 0, 5, 0 |
| map2 (+) id0 id1 | 1, 4, 2, 5, 3 |

Finally
#+BEGIN_SRC futhark
  let idxs2 = map2 (+) idxs0 idxs1
  let idxs = map (\x -> x-1) idxs idxs2
  let xs' = scatter (copy xs idxs xs
		     in xs
#+END_SRC
Work = O(n)
Span = O(log n)

#+BEGIN_SRC futhark
  -- Sorting xs: [n] u32
  -- 1 if bit b is set
  let check_bit b x =
    (i64.u32 (x >> u32.i32 b)) & 1

  let radix_step xs b =
      let bits = map(check_bit b) xs
      let bits_neg = map (1-) bits
      let offs = reduce (x) 0 bits_neg -- How many bits are not set
      let idxs0 = map2 (*)
		      bits_neg
		      (scan (+) 0 bits_neg)
      let idxs1 = map2(*)
		      bits
		      (map (+offs) (scan  (+) 0 bits))
      let idxs2 = map2 (+) idxs0 idxs1
      let idxs = map (\x -> x-1) idxs idxs2
      let xs' = scatter (copy xs idxs xs
		      in xs'
  let radix_sort [n] (xs: [n]u32): [n]u32 =
    loop xs for i < 32 do radix_sort_step xs i
					
#+END_SRC
* 23/11 Regular flattening
In futhark we work on "unzipped" soacs. Every soac accept multiple array inputs and
produce unzipped results.

We look at
#+BEGIN_SRC futhark
  let increment [n][m] (as: [n][m]i32) : [n]i32 =
    map (\r => map (+2) r) a
  let sum [n] (a: [n]i32) :
    reduce (+) 0 a
  let sumrows [n][m] (as: [n][m]i32) : [n]i32 =
    map sum as
	   
#+END_SRC
This pattern introduces a seperate intermediate matrix value in memory

If we are mapping a map: =map f (map g a)= is always equivalent to
=map (f o g) a= /only if/ no side effects are guaranteed. Loop fusion uses this

If we ever do a reduce on a map, this is an efficient pattern.

** Sidebar: Shorthand notaiton for sequences
$$\bar{z}^{(n)} = z_0, \dots, z_{n-1}$$
The n may be omitted. A seperator may be implied by context
$$ f \bar{v} \equiv f v1, \dots, v_n $$


When not all terms under the bar ar variant, subscript variant terms with /i/.
$$ (\bar{[d]v_i}) = ([d] ...$$

Convenient shorthands:
#+BEGIN_SRC 
redomap o f _d _xs = reduce o _d (map f _xs)
scanomap o f _d _xs = scan o _d (map f _xs)
#+END_SRC

** Nested parallelism
- GPUs have thousands of simple cores and taking full advantage of their compute power
  requires tens of thousands of threads.
- GPU threads are very restricted in what they can do: no stack, no allocation, limited control flow etc.
- Potential very high performance and lower power usage compared to CPUs.
- But programming them is /hard/

What follows is a PMPH Recap

SOACs kan be mapped onto a kernel  
More specifically, a map containing scalar code /or/ a reduce /or/ ... can be made into a kernel.  
It must be /perfectly nested/ aka the body must contain only one parallel operation.

** Nested parallelism
Now we have an issue. Futhark permits /nested/ paralllelism, but GPUs need flat parallel kernels.  
/Solution:/ Have the compiler rewrite program to perfectly nested *maps* containing sequential
code, or known parallel patterns such as segmented reduction.  
Example:
#+BEGIN_SRC futhark
  map (\xs -> let y = reduce (+) 0 xs
	      in map (\x -> x + y) xs)

  -- Rewrites to
  let ys = map (\xs -> reduce (+) 0 xs) xss
  in map (\xs y -> map (\x -> x + y) xs) xss ys
      
    
#+END_SRC
Having two perfectly nested parallel maps.

We can do loop fusion backwards via /loop fission/
The classic rule =map f o map g => map (f o g)= can be used backwards:
=map (f o g) => map f o map g=
#+BEGIN_SRC futhark

  let (asss, bss) =
    map (\(ps) ->
	   let = map(\(p): i32 ->
		       let cs = scan (+) 0 (iota p)
		       let r = reduce (+) 0 cs
		       in map (+r) ps) ps
	   let bs = loop ws=ps for i < n do
		      map (\as w: i32 ->
			     let d = reduce (+) 0 as
			     let e = d + w
			     in 2*e) ass ws
	   in (ass, bs)) pss
#+END_SRC
Start backwards First rewrite:

#+BEGIN_SRC futhark

  let (asss, bss) =
    map (\(ps) ->
	   let = map(\(p): i32 ->
		       let cs = scan (+) 0 (iota p)
		       let r = reduce (+) 0 cs
		       in map (+r) ps) ps
		    in ass) pss
#+END_SRC
I cannot follow his rewrites fast enough. Look this up later
*** TODO Rewrite

** Rules for flattening
Instead of writing:
#+BEGIN_SRC futhark
map (\ps rs ->
  map (\r -> 
    map (\p -> e)
      ps)
    rs)
  pss rss
#+END_SRC
We write
#+BEGIN_SRC 
segmap (<ps, rs \in pss rss>, <r \in rs>, <p \in ps>) e
#+END_SRC

For any number of maps, we use sugma and a set of angle brackets.

We also have segred Which is a bunch of maps on top of a redomap. Segscan === map map ... scanomap

How we can rewrite SOAC nests to these segmented operations

** Incremental flattening
We don't want to just flatten the outer loop and use no parallelism. We consider matrix mul
#+BEGIN_SRC futhark
  map (\xs ->
	 map (\ys ->
		let \s = map (*) xs ys
		in reduce (+) 0 zs)
	     (transpose yss))
      xss
#+END_SRC
Three levels of parallelism.

*** How to run this on a GPU
Full flattening
#+BEGIN_SRC futhark
  map (\xs ->
	 map (\ys ->
	     redomap (+) (*) 0 xs ys )
	     (transpose yss))
      xss
#+END_SRC
All parallelism explouted, however we have communication overhead.

Alternatively, only flatten the outer map. Run the redomap in sequence, which
allows us to optimize the redomap with more advanced stuff such as block tiling.  

There is no size fits all for parallelism

This is where /incremental flattening/. From a single source program, for each parallel construct
generate multiple semantically equivalent parallelisations, and generate a single program that at
runtime picks the least parallel that still saturates the hardware.
- Implemented in Futhark compiler
- But technique is applicable to any nested parallelism expressed
with the common Bird-Meertens-style array constructs.

At every level of map-nesting we have two options:
1. Continue flattening inside the map, exploiting the parallelism there
2. Sequentialise the map body; exploiting only the parallelism on top
   
If we look at matris multiplication, with n x p, and p x m size. We calculate $n * p > t_0$ 
where $t_0$ is the threshold parameter, which should be auto-tuned on the concrete hardware.
If we pass the threshold, we pick the lesser parallel version. If we don't we run the fully
parallel version.

* TODO 30/11 
* 02/12 ISPC
** SIMD
Simd. Same instruction, multiple data.  
In our notation, SIMD is regular and flat data parallellism on
vectors of static size.  

NB: Simd is cheap
- Already part of the cpu
- Using an ins has zero latency
  - Compared to GPUs which have huge overhead

NB2: SIMD is not multithreading.

In principle we can use SIMD whenever we have independent operations with the
same operator occur.

We can vectorise by strip-mining
#+BEGIN_SRC c
for (int i = 0; i < n; i++){
  x[i] *= 2;
}
// Becomes

for(int ii = 0; i < n; i += 4){
  // This gets unrolled and vectorid
  for(int i = ii; i < ii + 4; i++)
  {
    x[i] *= 2;
  }
}

#+END_SRC

*** An issue comes up. Vectorising scalar functions
We use masking just like in numpy. Turn control flow into data flow.  
- Doing this by hand gets tiring

** Auto-vectorisation
Write ordinary C code and hope that a sufficiently smart compiler
can figure out what you mean and make use of whatever instructions
your cpu supports.

Its hard in C. Intel tried. They introduced a Pragma. This is an admission
of "This problem is hard"

Matt pharr(ispc dude) came up with /SPMD/. Essentially the model used by
CUDA. 
- Multiple instancels of program is running
- Groyp of running instances is called a /gang/

We act as if every element in the inputs have their own program instance.

** COMMENT ISPC
- A variant of C
- A low-level parallel language
- Not guaranteed to be deterministic
- A compiler with only a few compiler optimisations

Example of using the right programming model. We mostly focus on high-level languages
but this is very low yet still automagically parallel

- Call to ispc spawns gang of program instances
- All instances are done when function returns.
- Gang size is SIMD width or a small multiple
- Lockstep execution
- Each instance gets a copy of each non-=uniform= variable

Example
#+BEGIN_SRC c
  export void f(uniform int n,
		uniform float as[]) {
    for (int i = programIndex;
	 i < n;
	 i += programCount){
      float a = as[i];
      // No barrier necessary. All in lockstep
      as[i] = a*2
    }
  }


#+END_SRC

ispc is concerned with real-world performance, not purity.
We might mess up and can create systems that break ispc. We just won't
in production.


